\documentclass[12pt,a4paper]{amsart}
\usepackage{a4wide}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage{graphics,graphicx,epsfig}
\usepackage{amssymb,amsfonts,amsthm,amsmath,mathtext,cite,enumerate,float}
\usepackage[english,russian]{babel}
\usepackage[all]{xy}
\usepackage{morefloats}
\usepackage{pgf}
\usepackage[debug,outputdir={docgraphs/}]{dot2texi}
\usepackage{tikz}
\usepackage{scalefnt}
\usepackage{listings}
\usepackage{float}
\usepackage{verbatim}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{decorations.pathmorphing}

% Comment the following block when compiling this .tex with a saner compiler than texlive.
\makeatletter
\def\@settitle{\begin{center}%
    \baselineskip14\p@\relax
    \bfseries
    \@title
  \end{center}%
}
\makeatother

\newtheorem{algo}{Алгоритм}
\newtheorem{theorem}{Теорема}
\newtheorem{stat}{Утверждение}
\newtheorem{defin}{Определение}

\begin{document}
% Comment the following block when compiling this .tex with a saner compiler than texlive.
\pagestyle{plain}
\lstset{language=C++}

\title{Алгоритмы порождения допустимых суперпозиций существенно нелинейных моделей}
\author{Г.\,И.~Рудой}
\address{Московский физико-технический институт, ФУПМ, каф. <<Интеллектуальные системы>>}
\thanks{Научный руководитель В.\,В.~Стрижов}

\begin{abstract}
  При восстановлении нелинейной регрессии предлагается рассмотреть набор
  индуктивно порожденных моделей с целью выбора оптимальной модели. В~работе
  исследуются индуктивные алгоритмы порождения допустимых существенно
  нелинейных моделей. Предлагается алгоритм, порождающий все возможные
  суперпозиции заданной сложности за конечное число шагов, и~приводится его
  теоретическое обоснование. В~вычислительном эксперименте приводятся
  результаты для задачи моделирования волатильности опционов.
\end{abstract}
\keywords{Символьная регрессия, нелинейные модели, индуктивное порождение нелинейных моделей, волатильность опционов}

\maketitle

\section{Введение}

В~ряде приложений \cite{Barmpalexis201175, Shi:2011:CRM, DOI:10.1504/IJCENT.2010.038358}
возникает задача восстановления регрессии по набору измеренных данных.
При этом предполагается, что модель должна иметь возможность быть
проинтерпретированной экспертом в~контексте предметной области.

Одним из методов, позволяющих получать интерпретируемые модели, является
символьная регрессия \cite{davidson:2000:snrea, reference/ml/X10vc},
согласно которой измеряемые данные приближаются некоторой математической
формулой, например, $ \sin x^2 + 2x $ или $\log x - \frac{e^x}{x} $.
При решении регрессионной задачи данные приближаются различными формулами,
являющимися произвольными суперпозициями функций из некоторого заданного
набора. Одна из возможных реализаций этого метода
предложена Джоном Коза \cite{Koza1998GP, Koza1998Intro}, использовавшим
эволюционные алгоритмы для реализации символьной регрессии. Иван Зелинка
предложил дальнейшее развитие этой идеи \cite{Zelinka2008}, получившее
название аналитического программирования.

Алгоритм построения требуемой математической модели выглядит следующим образом:
дан набор примитивных функций, из которых можно строить различные формулы
(например, степенная функция, $+$, $\sin$, $\tan$). Начальный набор формул
строится либо произвольным образом, либо на базе некоторых предположений
эксперта. Затем на каждом шаге производится оценка каждой из формул согласно
функции ошибки либо другого \footnote{Сходу не нашел публикаций на тему
использования других функционалов. Похоже, у Владиславлевой что-то было, но
пока не могу сослаться на что-то конкретное.} функционала качества. На базе
этой оценки у некоторой части формул случайным образом заменяется одна элементарная
функция на другую (например, $\sin$ на $\cos$ или $+$ на $\times$), а у некоторой
другой части происходит взаимный попарный обмен подвыражениями в~формулах.

Получаемая формула является математической моделью \cite{Pavlovsky2000}
исследуемого процесса или явления --- то есть, это математическое отношение,
описывающее основные закономерности, присущие этому явлению.

%DIPLOMA
%Среди возможных путей улучшения качества найденного решения задачи --- анализ
%информативности различных признаков. Например, в~ходе работы эволюционного
%алгоритма можно выявлять, какие из параметров слабо влияют на качество
%получающейся формулы, и~либо убирать их совсем, либо обеспечивать
%неслучайность замены элементарных функций или обмена поддеревьев с целью
%замены этих параметров на другие в~предположении, что они, возможно,
%окажутся более информативными.

Целью данной работы является теоретическое обоснование алгоритмов индуктивного
порождения моделей и~анализ этих алгоритмов. Другим вопросом, возникающим при
применении подобных эволюционных алгоритмов, является их принципиальная
теоретическая корректность: способен ли вообще такой алгоритм породить искомую
формулу.

Алгоритм индуктивного порождения моделей, предложенный в~настоящей работе, решает
некоторые типичные проблемы предложенных ранее методов, упомянутые, например,
в \cite{Zelinka2008}, а именно:
\begin{itemize}
  \item Порождение рекурсивных суперпозиций, суперпозиций, содержащих
	несоответствующее используемым функциям число аргументов, и~т. д. --- в
	предложенном алгоритме эти проблемы не возникают по построению.
  \item Несовпадение области определения некоторой примитивной функции и области
	значений ее аргументов (возможно, тоже некоторых суперпозиций).
  \item При ограничении числа примитивных функций, участвующих в~суперпозиции,
	а также при соответствующем задании множества примитивных функций
	исключается проблема слишком сложных суперпозиций.
\end{itemize}

Во~второй части данной работы формально поставлена задача построения алгоритма
индуктивного порождения моделей. Затем, в~третьей части строится искомый
алгоритм для частного случая непараметризованных моделей и~доказывается его
корректность, а затем алгоритм обобщается на случай моделей, имеющих параметры.
В~четвертой части описываются вспомогательные технические приемы, использованные
в~практическом алгоритме порождения моделей, описанном в~пятой части. Результаты
вычислительного эксперимента приведены в~шестой части настоящей работы.

\section{Постановка задачи}

Пусть дана регрессионная выборка:
\[
D = \{ (\mathbf{x}_i, y_i) \mid i \in \{1, \dots, N\},
			\mathbf{x}_i \in \mathbb{X} \subset \mathbb{R}^n,
			y_i \in \mathbb{Y} \subset \mathbb{R} \},
\]
где $N$ --- размерность регресионной выборки (число измерений), $\mathbf{x}_i$
--- вектор значений независимых переменных в $i$-ом измерении, $y_i$ ---
значение зависимой переменной в $i$-ом измерении, $\mathbb{X}$ --- множество
значений независимых переменных, лежащее в $\mathbb{R}_n$, $\mathbb{Y}$ ---
множество значений зависимой переменной.

Требуется построить функцию $f : \mathbb{R}^n \rightarrow \mathbb{R}$,
представленную суперпозицией примитивных функций из заданного множества $G$
и~доставляющую минимум некоторому функционалу ошибки, определяемому ниже.

То есть, если множество всех суперпозиций:
\[
\mathcal{F} = \{ f_r \mid
			f_r : (\boldsymbol{\omega}, \mathbf{x}) \mapsto y \in \mathbb{Y},
			r \in \mathbb{N} \},
\]
то требуется найти:
\begin{equation}
  \label{eq:hat_r}
  \hat{r} = \arg \min_{r \in \mathbb{N}} S (f_r \mid \boldsymbol{\hat{\omega}_r}, D),
\end{equation}
где
\begin{equation}
  \label{eq:hat_omega}
  \boldsymbol{\hat{\omega}_r} = \arg \min_{\boldsymbol{\omega} \in \Omega} S(\boldsymbol{\omega} \mid f_r, D).
\end{equation}

$\eqref{eq:hat_omega}$ означает, что мы сначала для каждой из суперпозиций
$f_r$ находим оптимальный вектор параметров $\hat{omega}_r$ при
данной регрессионной выборке, а $\eqref{eq:hat_r}$ означает,
что среди всех $f \in \mathcal{F}$ мы выбираем суперпозицию $f_{\hat{r}}$,
доставляющую минимум функционалу качества $S$ при данной регрессионной
выборке.

В качестве функционала качества $S$ используется SSE:
\begin{equation}
  \label{eq:sse_expr}
  S(\boldsymbol{\omega}, f, D) = \sum_{i=1}^N (y_i - f (\boldsymbol{\omega}, \mathbf{x}_i))^2 \mid (\mathbf{x}_i, y_i) \in D.
\end{equation}

Пусть $G = \{ g_1, \dots, g_l \}$ --- множество данных примитивных
функций, а именно, для каждой $g_i \in G$ заданы:
\begin{itemize}
  \item сама функция $g_i$ (например, $\sin$, $\cos$, $\times$),
  \item арность функции и~порядок следования аргументов,
  \item <<полная>> область определения и~<<полная>> область значений функции
	(например, $\mathbb{R} \to \mathbb{R}$ или $\mathbb{R}^2 \to \{0, 1\}$),
  \item область определения и~область значений в~рамках соответствующих
	<<полных>> областей (например, для $log_{x_1} x_2 : x_1 \in (0; 1) \cup (1; +\infty), x_2 \in (0; +\infty)$).
\end{itemize}

Поясним различие между последними двумя пунктами. Например, <<полная>>
область определения показывает, значения из какого множества принимает
функция (целые числа, действительные числа, декартово произведение целых
чисел и $\{0, 1\}$, и~т.~п.). Просто область определения же показывает,
на каких значениях из этого множества функция определена и имеет смысл.

Требуется также:
\begin{itemize}
  \item построить алгоритм $\mathfrak{A}$, за конечное число итераций
	порождающий любую конечную суперпозицию данных примитивных функций,
  \item указать способ проверки изоморфности двух суперпозиций.
\end{itemize}

Заметим, что мы не требуем для примитивных функций свойства их
непорождаемости в~наиболее общей формулировке типа принципиальной
невозможности породить в~ходе работы искомого алгоритма суперпозицию,
изоморфную некоторой функции из $G$. Такое требование является слишком
ограничивающим. В~частности, невозможно было бы иметь в~$G$ одновременно,
например, функции $\text{id}$, $\exp$ и~$\log$, так как
$\text{id} \equiv \log \circ \exp$.

\section{Пути решения задачи: теоретическая часть}

%DIPLOMA
\begin{comment}
Введем некоторые понятия из теории графов:

\begin{defin}
  Дерево --- связный ациклический граф.
\end{defin}

Иными словами, дерево $\Gamma$ --- это некоторый набор вершин $V_i$ и
соединяющих их ребер $E(U, V)$ обладающий следующими свойствами:

\begin{itemize}
  \item Из каждой вершины $V_i$ можно попасть в~каждую другую вершину
	$V_j, j \neq i$ (условие связности).
  \item Для любой вершины $V_i$ не существует пути из нее в~нее же
	(условие ацикличности).
\end{itemize}

\begin{defin}
  Ориентированный граф (орграф) --- граф, в~котором каждому ребру $E$
  сопоставлено направление.
\end{defin}

Ребра в~орграфах также называются \emph{дугами}.

\begin{defin}
  Ориентированное дерево --- связный ациклический орграф, в~котором
  только одна вершина не имеет входящих в~нее дуг, а все остальные
  вершины имеют только одну входящую в~них дугу.
\end{defin}

Заметим, что для ориентированного дерева условие связности следует
уточнить. А именно, связность означает, что для любой пары различных
вершин $U$ и~$V$ существует путь между ними без учета направления.

В~дальнейшем мы будем работать в~основном с ориентированными деревьями,
поэтому для краткости будем называть их просто деревьями. В~случае
необходимости неориентированность будет указана явно.

Условимся называть вершину дерева, не имеющую входящих в~нее дуг,
\emph{корнем дерева}:

\begin{defin}
  Корень дерева --- вершина дерева, в~которую не входит ни одна дуга.
\end{defin}

Условимся обозначать вершину дерева $V_0$.

\begin{defin}
  Лист дерева --- вершина, из которой не исходит ни одной дуги.
\end{defin}

\begin{defin}
  Уровень вершины --- длина пути от корня дерева до данной вершины.
\end{defin}

Заметим, что каждой вершине $V_i$ мы можем поставить в~соответствие
поддерево $\Gamma^i$, убрав дугу, приходящую в~$V_i$, и~убрав все вершины,
недостижимые из $V_i$. Корнем такого дерева, очевидно, является $V_i$.

Введем понятия множества смежных вершин для данной вершины:

\begin{defin}
  Множество смежных вершин $S(U)$ для вершины $U$ --- вершины, в~которые
  входит некоторая дуга, выходящая из $U$. Иными словами:
  $S(U) = \{ V \mid \exists E (U, V) \}$.
\end{defin}

Сформулируем алгоритм поиска в~глубину применительно к~ориентированным
деревьям. Будем считать, что вершины одного уровня расположены в~некотором
заранее заданном порядке.

\begin{algo}
  Алгоритм поиска в~глубину (Depth-First Search, $\mathbf{DFS}$).
\end{algo}
\begin{lstlisting}
  map<Vertex, int> DFS (Vertex v, int current)
  {
	map<Vertex, int> result;
	result [v] = current;

	auto children = S (v);

	for (Vertex child : children)
	  result = Unite (result, DFS (child, ++current));

	return result;
  }

  DFS (v0, 0);
\end{lstlisting}

Отметим, что в~общем случае алгоритм $\mathbf{DFS}$ строится более сложным
образом для учета циклов и~случаев несвязных графов.

\begin{figure}[H]
  \begin{tikzpicture}
	\scalefont{2}
	\tikzstyle{n} = [draw, inner sep=2pt, fill=red!20]
	\begin{dot2tex}[dot,options=-tmath,scale=0.5]
	  digraph G1 {
		node [shape="circle",style="n"];
		N1 [label="1"];
		N2 [label="2"];
		N3 [label="3"];
		N4 [label="4"];
		N5 [label="5"];
		N6 [label="6"];
		N7 [label="7"];
		N8 [label="8"];
		N9 [label="9"];

		N1 -> N2;
		N2 -> N3;
		N2 -> N4;
		N1 -> N5;
		N5 -> N6;
		N5 -> N7;
		N7 -> N8;
		N5 -> N9;
	  }
	\end{dot2tex}
  \end{tikzpicture}
  \caption{Порядок обхода вершин при поиске в~глубину}
\end{figure}

\end{comment}

Условимся считать, что каждой суперпозиции $f$ сопоставлено дерево $\Gamma_f$,
эквивалентное этой суперпозиции и~строящееся следующим образом:

\begin{itemize}
  \item В~вершинах $V_i$ дерева $\Gamma_f$ находятся соответствующие
	примитивные функции $g_s, s = s(i)$.
  \item Число дочерних вершин у некоторой вершины $V_i$ равно арности
	соответствующей функции $g_s$.
  \item Порядок смежных некоторой вершине $V_i$ вершин соотвествует порядку
	аргументов соответствующей функции $g_{s(i)}$.
  \item В~листьях дерева $\Gamma_f$ находятся свободные переменные либо
	константы.
  \item Порядок вершин $V_i$ в~смысле уровня вершин определяет порядок
	вычисления примитивных функций: дерево вычисляется снизу вверх.
	То есть, сначала подставляются конкретные значения свободных переменных,
	затем вычисляются значения в~вершинах, все дочерние вершины которых ---
	свободные переменные, и~так далее до тех пор, пока не останется
	единственная вершина, бывшая корнем дерева, содержащая результат выражения.
\end{itemize}

Таким образом, вычисление значения выражения $f$ в~некоторой точке эквивалентно
подстановке соответствующих значений свободных переменных $x_i$ в~дерево $\Gamma_f$
выражения.

Заметим важное свойство таких деревьев: каждое поддерево $\Gamma_f^i$
дерева $\Gamma_f$, соответствующее вершине $V_i$, также соответствует
некоторой суперпозиции, являющейся составляющей исходной суперпозиции $f$.

Для примера рассмотрим дерево, соответствующиее суперпозиции
$\sin (\ln x_1) + \frac{x_2^3}{2}$ (см. рис \ref{fig:expr_tree_example}).

\begin{figure}[h]
  \begin{tikzpicture}
	\scalefont{2}
	\tikzstyle{n} = [draw, inner sep=2pt, fill=red!20]
	  \begin{dot2tex}[dot,options=-tmath,scale=0.5]
		digraph G1 {
		  node [shape="circle",style="n"];
		  
		  Plus [label="\bullet + \bullet"];
		  Sin [label="\sin \bullet"];
		  Ln [label="\ln \bullet"];
		  X1 [label="x_1"];
		  Frac [label="\div"];
		  Pow [label="\bullet^{\bullet}"];
		  X2 [label="x_2"];
		  N3 [label="3"];
		  N2 [label="2"];

		  Plus -> Sin;
		  Sin -> Ln;
		  Ln -> X1;

		  Plus -> Frac;
		  Frac -> Pow;
		  Frac -> N2;

		  Pow -> X2;
		  Pow -> N3;
		}
	  \end{dot2tex}
	\end{tikzpicture}
  \caption{Дерево выражения $\sin (\ln x_1) + \frac{x_2^3}{2}$}
  \label{fig:expr_tree_example}
\end{figure}

Здесь точками обозначены аргументы функций. Как видно, корнем дерева является
вершина, соответствующая операции сложения, которая должна быть выполнена
в~последнюю очередь. Операция сложения имеет два различных поддерева,
соответствующих двум аргументам этой операции. Заметим также, что здесь не
использованы операции типа <<разделить на два>> или <<возвести в~куб>>.
Вместо этого используются операции деления и~возведения в степень в~общем
виде, а в~данном конкретном дереве соответствующие аргументы зафиксированны
соответствующими константами.

В~дальнейшем будем также считать, что суперпозиция, соответствующая
единственной свободной переменной ($f(\mathbb{x}) = x_i$), полностью
эквивалентна функции вида $\text{id} x_i$.

\subsection{Алгоритм порождения суперпозиций}

Сначала определим понятие \emph{глубины суперпозиции}:

\begin{defin}
  Глубина суперпозиции $f$ --- максимальная глубина дерева $\Gamma_f$.
\end{defin}

Итак, пусть дано множество примитивных функций $G = \{ g_1, \dots, g_l \}$ и
множество свободных переменных $X = \{ x_1, \dots, x_n \}$. Сначала
опишем итеративный алгоритм, порождающий суперпозиции, не содержащие числовых
параметров. Описанный алгоритм породит любую суперпозицию конечной глубины
за конечное число шагов.

Для удобства будем исходить из предположения, что множество $G$ состоит
только из унарных и~бинарных функций, и~разделим его соответствующим образом
на два подмножества:
$G = G_b \cup G_u \mid G_b = \{ g_{b_1}, \dots, g_{b_k} \}, G_u = \{ g_{u_1}, \dots, g_{u_l} \}$,
где $G_b$ --- множество всех бинарных функций, а $G_u$ --- множество всех
унарных функций из $G$. Потребуем также наличия $\text{id}$ в~$G_b$.

\begin{algo}
  Алгоритм $\mathfrak{A}$ итеративного порождения суперпозиций.
\end{algo}
\begin{enumerate}
  \item Инициализируем вспомогательное множество $\mathcal{I}_f = \{ (x, 0) \mid x \in X \}$,
	служащее для запоминания, на какой итерации была впервые встречена
	каждая суперпозиция.
  \item Инициализируем множество $\mathcal{F}_0 = X$.
  \item Для множества $\mathcal{F}_i$ построим вспомогательное множество $U_i$,
	состоящее из результатов применения функций из $G_u$ к~элементам $\mathcal{F}_i$:
	\[
	U_i = \{ g_u \circ f \mid g_u \in G_u, f \in \mathcal{F}_i \}
	\]
  \item Аналогичным образом построим вспомогательное множество $B_i$ для
	бинарных функций:
	\[
	B_i = \{ g_b \circ (f, h) \mid g_b \in G_b, f, h \in \mathcal{F}_i \}
	\]
  \item Обозначим $\mathcal{F}_{i+1} = \mathcal{F}_i \cup U_i \cup B_i$.
  \item Для каждой суперпозиции $f$ из $\mathcal{F}_{i+1}$ добавим пару
	$(f, i+1)$ в~множество $\mathcal{I}_f$, если суперпозиция $f$ еще там
	не присутствует.
  \item Перейдем к~следующей итерации. 
\end{enumerate}

Тогда $\mathcal{F} = \cup_0^\infty \mathcal{F}_i$ --- множество всех
возможных суперпозиций конечной длины, которые можно построить из
данного множества примитивных функций.

Вспомогательное множество $\mathcal{I}_f$ позволяет запоминать, на какой
итерации была впервые встречена данная суперпозиция. Это необходимо, так
как каждая суперпозиция, впервые порожденная на $i$-ой итерации, будет
порождена еще раз и~на любой итерации после $i$.

Алгоритм $\mathfrak{A}$ очевидным образом обобщается на случай, когда
множество $G$ содержащит функции произвольной (но конечной) арности.
Действительно, для такого обобщения достаточно строить аналогичным образом
вспомогательные множества для этих функций, а именно, для множества функций
$G_n$ арности $n$ построим вспомогательное множество $H_i^n$ вида:
\[
H_i^n = \{ g \circ (f_1, f_2, \dots, f_n) \mid g \in G_n, f_j \in \mathcal{F}_i \}.
\]

В~этих обозначениях $U_i \equiv H_i^1$, а $B_i \equiv H_i^2$.

Тогда множество $\mathcal{F}_{i+1} = \mathcal{F}_i \cup_{n=0}^{n_{max}} H_i^n$,
где $n_{max}$ --- максимальное значение арности функций из $G$.

\begin{theorem}
  Алгоритм $\mathfrak{A}$ действительно породит любую конечную суперпозицию
  за конечное число шагов.
\end{theorem}
\begin{proof}
  Чтобы убедиться в~этом, найдем номер итерации, на котором будет порождена
  некоторая произвольная конечная суперпозиция $f$. Чтобы найти этот номер,
  составим цепочку соотношений на номера итераций, на которых будут получены
  компоненты суперпозиции $f$. По факту, эта цепочка соотношений является
  системой равенств, связывающая номера итераций, на которых были впервые
  порождены связанные друг с другом узлы в графе $\Gamma_f$ суперпозиции.
  Для того, чтобы составить такой набор равенств, представим суперпозицию
  $f$ в~виде соответствующего графа $\Gamma_f$ и~рекурсивно пройти от вершин
  к~листьям, составляя цепочку соотношений по следующим правилам:

  \begin{itemize}
	\item Если вершина $V$, полученная на $i$-ой итерации —-- унарная функция,
	  то это функция от выражения, полученного на $(i-1)$-ой итерации.
	\item Если вершина $V$, полученная на $i$-ой итерации --- бинарная функция,
	  то это функция от двух выражений, как минимум одно из которых получено
	  на $(i-1)$-ой итерации, а другое --- на $(i-1)$-ой или ранее.
	\item Если это лист со свободной переменной, то он получен на нулевой
	  итерации.
  \end{itemize}

  При помощи этой цепочки соотношений можно получить номер итерации, на
  которой суперпозиция $f$ была порождена.
  
  Иными словами, для любой суперпозиции мы можем указать конкретный номер
  итерации, на котором она будет получена, что и~требовалось.
\end{proof}

В~предложенных ранее методах\cite{Zelinka2008} построения суперпозиций
необходимо было самостоятельно следить за тем, чтобы в~ходе работы алгоритма
не возникало <<зацикленных>> суперпозиций типа $f(x, y) = g (f(x, y), x, y)$.
Заметим, что в~предложенном алгоритме $\mathfrak{A}$ такие суперпозиции
не могут возникнуть по построению.

\subsection{Порождение параметризованных моделей}
Алгоритм в~таком виде не позволяет получать выражения, содержащие численные
параметры регрессионной модели. Покажем, однако, на примере конструирования
множеств $U_i$ и~$B_i$, как исходный алгоритм может быть расширен с учетом
таких параметров путем введения параметров:
\[
U_i = { g_u \circ (\alpha f + \boldsymbol{\omega}) },
\]
\[
B_i = { g_b \circ (\alpha f + \boldsymbol{\omega}, \psi h + \phi) }.
\]

Будем обозначать этот расширенный алгоритм как $\mathfrak{A}^*$.

Здесь параметры $\alpha, \boldsymbol{\omega}$ зависят только от комбинации
$g_u, f$ (или $g_b, f, h$ для $\alpha, \boldsymbol{\omega}, \psi, \phi$).
Соответственно, для упрощения их индексы опущены.

Иными словами, мы предполагаем, что каждая суперпозиция из предыдущих итераций
входит в~следующую, будучи умноженной на некоторой коэффициент и~с константной
поправкой.

Очевидно, при таком добавлении параметров $\alpha, \boldsymbol{\omega}, \psi, \phi$
мы не изменяем мощности получившегося множества суперпозиций, поэтому
алгоритм и~выводы из него остаются корректны. В~частности, исходный алгоритм
является частным случаем данного при
$\alpha \equiv \psi \equiv 1, \boldsymbol{\omega} \equiv \phi \equiv 0$.

$\alpha, \boldsymbol{\omega}, \psi, \phi$ являются параметрами модели. В
практических приложениях можно оптимизировать значения этих параметров у
получившихся суперпозиций, например, алгоритмом Левенберга-Марквардта
\cite{Marquardt1963Algorithm, more:78}.

Заметим также, что такая модификация алгоритма позволяет нам получить единицу,
например, для построения суперпозиций типа $\frac{1}{x}$:
$1 = \alpha\ id\ x + \boldsymbol{\omega} \mid \alpha = 0, \boldsymbol{\omega} = 1$.

Отдельно подчеркнем, что численные параметры у различных суперпозиций
различны. Однако, так как на разных итерациях алгоритма мы можем получить,
вообще говоря, одну и~ту же суперпозицию с точностью до этих параметров,
их необходимо не учитывать при тестировании различных суперпозиций на
равенство.

Кроме того, опять же, заметим, что и~этот алгоритм очевидным образом
обобщается на случай множества $G$, содержащего функции произвольной арности.

\subsection{Количество возможных суперпозиций}

Посчитаем количество суперпозиций, получаемых после каждой итерации алгоритма
$\mathfrak{A}$. Очевидно, с учетом вышеупомянутых оговорок касательно сравнения
параметризованных суперпозиций, это количество равно количеству для алгоритма
$\mathfrak{A*}$.

Итак, пусть дано $n$ независимых переменных: $| X | = n$, а мощность
множества $G$ распишем через мощности его подмножеств функций соответствующей
арности: $| G_1 | = l_1, | G_2 | = l_2, \dots, | G_p | = l_p$. На нулевой
итерации имеем $S_0 = n$ суперпозиций.

На первой итерации дополнительно порождается:
\[
S_1 = l_1 n + l_2 n^2 + \dots + l_n n^p = \sum_{i=1}^p l_i S_0^i,
\]
и суммарное число суперпозиций после первой итерации:
\[
\hat{S}_1 = S_1 + S_0 = \sum_{i=1}^p l_i S_0^i + S_0.
\]

Как было замечено ранее, суперпозиции, порожденные на $k$-ой итерации, будут
также порождены и~на любой следующей после $k$ итерации, поэтому суммарное
число суперпозиций после второй итерации будет равно:
\[
\hat{S}_2 = \sum_{i=1}^p l_i \hat{S}_1^i.
\]

И вообще, после $k$-ой итерации будет порождено:
\[
\hat{S}_k = \sum_{j=1}^p l_i \hat{S}_{k-1}^i.
\]

Оценим порядок роста количества функций, порожденных после $k$-ой итерации.

\begin{theorem}
  Пусть в множестве примитивных функций $G$ содержится $l_p$ функций арности
  $p > 1$ и ни одной функции арности $p + k \mid k > 0$, и имеется $n > 1$
  независимых переменных. Тогда справедлива следующая оценка количества
  суперпозиций, порожденных алгоритмом $\mathfrak{A}$ после $k$-ой итерации:
  \[
  | \mathcal{F}_k | = \mathcal{O} (l_p^{\sum_{i=0}^{k-1} p^i} n^{p^k}).
  \]
\end{theorem}
\begin{proof}
  Оценим сначала порядок роста для случая, когда есть лишь одна $m$-арная
  функция и~$n$ свободных переменных.

  После первой итерации алгоритма будет порождено $n^m + n$ суперпозиций.
  После второй --- $(n^m + n)^m + n^m + n$, что можно оценить как 
  $(n^m)^m = n^{m^2}$. И вообще, после $k$-ой итерации количество
  суперпозиций можно оценить как $n^{m^k}$.

  Видно, что для оценки скорости роста количества порожденных суперпозиций
  можно учитывать только функции с наибольшей арностью.

  Рассмотрим теперь случай, когда имеется не одна функция арности $m$, а
  $l_m$ таких функций. Тогда на первой итерации порождается $l_m n^m + n$
  суперпозиций, на второй:
  \[
  l_m (l_m n^m + n)^m + l_m n^m + n \approx l_m^{m+1} n^{m^2},
  \]
  на третьей, с учетом этого приближения
  \[
  l_m (l_m^{m+1} n^{m^2})^m = l_m l_m^{m(m+1)} n^{m^3} = l_m^{m^2 + m + 1} n^{m^3}.
  \]

  И вообще, скорость роста количества порожденных суперпозицийможно оценить
  как:
  \[
  | \mathcal{F}_k | = \mathcal{O} (l_m^{\sum_{i=0}^{k-1} m^i} n^{m^k}).
  \]

  Таким образом, получаем оценку для случая, когда в множестве $G$ содержится
  $l_p$ функций арности $p$ и ни одной функции арности $p + k \mid k > 0$:
  \[
  | \mathcal{F}_k | = \mathcal{O} (l_p^{\sum_{i=0}^{k-1} p^i} n^{p^k}).
  \]
\end{proof}

\subsection{Множество допустимых суперпозиций}

Предложенный выше алгоритм позволяет получить действительно все возможные
суперпозиции, однако, не все они будут пригодны в~практических приложениях:
например, $\ln x$ имеет смысл только при $x > 0$, а $\frac{x}{0}$ не имеет
смысла вообще никогда. Выражения типа $\frac{x}{\sin x}$ имеют смысл только
при $x \neq \pi k$.

Таким образом, необходимо введение понятия множества \emph{допустимых}
суперпозиций --- то есть, таких суперпозиций, которые в~условиях некоторой
задачи корректны.

\begin{defin}
  Допустимая суперпозиция $f$ --- такая суперпозиция, значение которой
  определено для любой комбинации значений свободных переменных , область
  значений $\mathbb{X}$ которых определяется конкретной задачей,
  $\mathbb{X} \subset \mathbb{R}^n$ где $n$ --- число свободных переменных.
\end{defin}

Одним из способов построения только допустимых суперпозиций является
модификация предложенного алгоритма таким образом, чтобы отслеживать
совместность областей определения и~областей значения соответствующих
функций в~ходе построения суперпозиций. Для свободных переменных это будет,
в свою очередь, означать необходимость задания областей значений
$\mathcal{X}$ пользователем при решении конкретных задач.

Заметим, что, хотя теоретически возможно выводить допустимость выражений
вида $\frac{x}{\sin x}$ исходя из заданных условий на свободную переменную
(например, что $x \in (\frac{\pi}{4}, \frac{\pi}{2})$), в~общем случае это
потребует решения неравенств в~общем виде, что вычислительно неэффективно.

Таким образом, можно сформулировать очевидное \emph{достаточное условие
недопустимости} суперпозиции:

\begin{defin}
  Достаточное условие недопустимости суперпозиции $f$: в~соответствующем дереве
  $\Gamma_f$ хотя бы одна вершина $V_i$ имеет хотя бы одну дочернюю вершину
  $V_j$ такую, что область значений функции $g_{s(j)}$ шире, чем область
  определения функции $g_{s(i)}$.
\end{defin}

Говоря, что область значений функции $f$ шире области определения функции
$g$, мы имеем ввиду, что существует по крайней мере одно значение функции
$f$, не входящее в область определения функции $g$.

Подчеркнем, что, хотя свободные переменные могут принимать, например, все
значения из $\mathbb{R}$, выбором множества $\mathbb{X}$ можно обеспечить
возможность использования их в качестве аргументов функциям с более узкой,
чем $\mathbb{R}$, но не менее узкой, чем $\mathbb{X}$, областью определения,
если это не противоречит данной регрессионной выборке.

Для построения множества допустимых суперпозиций достаточно построить
множество всех возможных суперпозиций при помощи алгоритма $\mathfrak{A}^*$,
а затем удалить из этого множества все суперпозиции, не удовлетворяющие
сформулированному признаку.

\subsection{Множество <<минимальных>> суперпозиций}

В~ходе работы алгоритма могут возникать суперпозиции вида $x + x$ и~$2x$,
и хотя эти выражения эквивалентны, они представляются различными формулами.
Аналогично эквивалентны $x + y$ и~$y + x$, отличающиеся порядком следования
слагаемых. Таким образом, необходим способ нормализации суперпозиций.

Во-первых, необходимо обеспечивать одинаковый порядок следования операндов,
например, упорядочивая их каким-либо образом у коммутирующих бинарных функций.

Во-вторых, необходимо иметь набор правил, позволяющих проверить равенство
$x + x$ и~$2x$. Иными словами, необходимо иметь набор связей между различными
функциями из множества данных примитивных функций. Заметим, что в~общем
случае эта задача требует введения значительного числа правил и~по определению
сводится к~последовательному переборному их применению к~различным
подвыражениям суперпозиции.

В~связи с этим может оказаться более эффективным иной подход к~сравнению
суперпозиций: так как по условию практической задачи значения искомой функции
даны в~конечном числе точек, то для проверки на равенство достаточно вычислить
получившиеся суперпозиции в~этих точках и~сравнить их.

Другим способом, позволяющим избежать разрастания количества правил, может
являться использование только <<независимых>> функций. Например, $\sin$ и
$\cos$ связаны известным тригонометрическим соотношением с точностью до знака,
а значит, $\sin$ и~$\tan = \frac{\sin}{\cos}$ также связаны, как и~ряд прочих
тригонометрических функций, поэтому предлагается среди примитивных функций
оставить лишь $\sin$ и~стандартные арифметические действия для вывода прочих
тригонометрических функций через соответствующие соотношения.

Однако, можно заметить два часто встречающихся шаблона правил, связывающих
различные функции:
\begin{itemize}
  \item Для унарных функций это $f \circ g = h$ (например,
	$\ln \circ \exp = id$).
  \item Для бинарных функций это $ f (x, g (x, i)) = g (x, s (i)) $.
	Например, $x + xi = x(i+1)$: здесь $f = (+), g = (\times), s(i) = i + 1$.
\end{itemize}

В~практических приложениях представляется целесообразным использование
набора правил такого вида вкупе с использованием только <<независимых>>
тригонометрических функций, то есть, по факту, какой-нибудь одной из них
и еще одной обратной.

\section{Алгоритм Левенберга-Марквардта и~мультистарт}

Алгоритм Левенберга-Марквардта ($\mathcal{LM}$) \cite{Marquardt1963Algorithm, more:78}
предназначен для решения задачи минимизации функции, представляющей из себя
сумму квадратичных членов. В~частности, он используется для оптимизации
параметров нелинейных регрессионных моделей в~предположении, что в~качестве
критерия оптимизации используется среднеквадратичная ошибка модели на
обучающей выборке:
\[
S(\boldsymbol{\omega}) = \sum_{i=1}^{N} [y_i - f(\boldsymbol{\omega}, \mathbf{x}_i)]^2 \to \min,
\]
где $\boldsymbol{\omega}$ --- вектор параметров перпозиции $f$.

$\mathcal{LM}$ может рассматриваться как комбинация методов Гаусса-Ньютона и
градиентного спуска.

Перед началом работы алгоритма задается начальный вектор параметров $\boldsymbol{\omega}_0$.
На каждой итерации этот вектор заменяется новой оценкой,
$\boldsymbol{\omega}_{k+1} = \boldsymbol{\omega}_k + \boldsymbol{\delta}_k$.
Для определения $\boldsymbol{\delta}_k = \boldsymbol{\delta}$ используется линейное приближение функции:
\[
\mathbf{f(\boldsymbol{\omega} + \boldsymbol{\delta}, X)} \approx
	\mathbf{f(\boldsymbol{\omega}, X)} + \mathbf{J} \boldsymbol{\delta},
\]
где $\mathbf{J}$ --- якобиан функции $\mathbf{f}$ в~точке $\boldsymbol{\omega}$.

Приращение $\boldsymbol{\delta}$ в~точке $\boldsymbol{\omega}$, доставляющей минимум $S$,
равно нулю, поэтому для нахождения последующего значения приращения $\boldsymbol{\delta}$
приравняем нулю вектор частных производных $S$ по $\boldsymbol{\omega}$. То есть,
в векторной нотации:
\[
S(\mathbf{\boldsymbol{\omega} + \boldsymbol{\delta}}) \approx \| \mathbf{y - f (\boldsymbol{\omega}) - J\boldsymbol{\delta}} \|^2.
\]

Дифференциирование по $\boldsymbol{\delta}$ и~приравнивание нулю приводит к
следующему уравнению для $\boldsymbol{\delta}$:
\[
(\mathbf{J}^T\mathbf{J})\boldsymbol{\delta} = \mathbf{J}^T [\mathbf{y - f(\boldsymbol{\omega})}].
\]

Левенберг предложил заменить $(\mathbf{J}^T\mathbf{J})$ на
$(\mathbf{J}^T\mathbf{J} + \lambda\mathbf{I})$, где $\lambda$ --- некоторый
параметр регуляризации. Марквардт дополнил это предложение с целью более
быстрого движения по тем направлениям, где градиент меньше. Для этого вместо
$\mathbf{I}$ используется диагональ матрицы $\mathbf{J}^T\mathbf{J}$, и
искомое уравнение на $\boldsymbol{\delta}$ выглядит как:
\[
(\mathbf{J}^T\mathbf{J} + \lambda \text{diag} (\mathbf{J}^T\mathbf{J}))\boldsymbol{\delta} = \mathbf{J}^T [\mathbf{y - f(\boldsymbol{\omega})}].
\]

Решая это уравнение, получаем окончательное выражение для
$\boldsymbol{\delta} = \boldsymbol{\delta}_k$:
\[
\boldsymbol{\delta}_k = (\mathbf{J}^T\mathbf{J} + \lambda \text{diag} (\mathbf{J}^T\mathbf{J}))^{-1} \mathbf{J}^T [\mathbf{y - f(\boldsymbol{\omega})}].
\]

\subsection{Выбор $\lambda$}

Для определения параметра регуляризации $\lambda$ в~настоящей работе
применяется следующая эвристика.

В~начале работы $\mathcal{LM}$ задается некоторое значение $\lambda_0$,
например, $0.01$, и~фиксируется коэффициент $\nu > 1$. Затем, на каждой
итерации алгоритма вычисляется значение функционала ошибки для
$\lambda = \lambda_i$ и~$\lambda = \frac{\lambda_i}{\nu}$. В~случае,
если хотя бы одно из этих значений доставляет функционалу ошибки
меньшее значение, чем до этой итерации, то $\lambda_{i+1}$ принимается
равным этому значению. Иначе $\lambda_i$ умножается на $\nu$ до тех
пор, пока значение функционала ошибки не уменьшится.

\subsection{Мультистарт}

Как и~всякий подобный алгоритм оптимизации, $\mathcal{LM}$ находит лишь
локальный минимум. Для решения этой проблемы применяется метод \emph{мультистарта}:
случайным образом задается несколько начальных приближений, и~для каждого из
них запускается $\mathcal{LM}$. Если найдено несколько различных локальных
минимумов, то выбирается тот из них, в~котором значение $S(\boldsymbol{\omega})$
меньше всего.

\section{Алгоритм итеративного стохастического порождения суперпозиций}

Несмотря на то, что указанный ранее итеративный алгоритм порождения
суперпозиций позволяет получить за конечное число шагов произвольную
суперпозицию, для практических применений он непригоден, как и~любой алгоритм,
реализующий полный перебор, в~связи с чрезмерной вычислительной сложностью.
Вместо него можно использовать стохастические алгоритмы и~ряд эвристик,
позволяющих на практике получать за приемлемое время результаты,
удовлетворяющие заранее заданным условиям <<достаточной пригодности>>.
В~данном разделе описывается примененный в~настоящей работе алгоритм.

Сначала опишем вспомогательный алгоритм случайного порождения суперпозиции:

\begin{algo}
  Алгоритм случайного порождения суперпозиции $\mathcal{RF}$.

  Вход:
  \begin{itemize}
	\item Набор пороговых значений $0 < \xi_1 < \xi_2 < \xi_3 < 1$.
	\item Максимальная глубина порождаемой суперпозиции $Td$.
  \end{itemize}
\end{algo}

Алгоритм работает следующим образом. Генерируется случайное число $\xi$ на
интервале $(0; 1)$, и рассматриваются следующие случаи:
\begin{itemize}
  \item $\xi \leq \xi_1$: результатом алгоритма является некоторая случайно
	выбранная свободная переменная.
  \item $\xi_1 < \xi \leq \xi_2$: результатом алгоритма является	числовой
	параметр.
  \item $\xi_2 < \xi \leq \xi_3$: результатом алгоритма является некоторая
	сулчайно выбранная унарная функция, для определения аргумента которой
	данный алгоритм рекурсивно запускается еще раз.
  \item $\xi_3 < \xi$: результатом алгоритма является некоторая случайно
	выбранная бинарная функция, аргументы которой порождаются аналогичны
	образом.
\end{itemize}

При этом, порождение тривиальных суперпозиций (свободных переменных и
параметров) запрещено: на самом первом шаге пороговые значения масштабируется
таким образом, чтобы всегда породилась унарная или бинарная функция.
Аналогично при превышении значения $Td$ пороговые значения масштабируются
таким образом, чтобы был порожден узел, соответствующий свободной переменной
или параметру, и алгоритм завершился.

В~ходе работы предлагаемого алгоритма каждой суперпозиции $f$ ставится в
соответствие ее \emph{приспособленность} $Q_f$ (иногда будем говорить, что
суперпозиция \emph{оценивается}), рассчитываемая исходя из функции ошибки
$S_f$ этой суперпозиции на обучающей выборке и ее сложности $C_f$ ---
числа узлов в соответствующем графе $\Gamma_f$, по следующей формуле:
\begin{equation}
  \label{eq:q_f}
  Q_f = \frac{1}{1 + S_f} \big(\alpha \hat{Q} + \frac{1 - \alpha \hat{Q}}{1 + \text{exp} (C_f - 10)}\big),
\end{equation}
где $\hat{Q}$ --- минимальная приспособленность суперпозиции из критерия
останова, а $\alpha$ --- некоторый коэффициент, $0 \ll \alpha < 1$.

Второй множитель в данной формуле выполняет роль штрафа за слишком
большую сложность суперпозиции.

Таким образом, чем лучше результаты суперпозиции, тем ближе значение ее
приспособленности к~$1$, и, наоборот, чем хуже --- тем ближе к~$0$.

Итак, теперь опишем сам алгоритм:
\begin{algo}
  Итеративный алгоритм стохастического порождения суперпозиций.

  Вход:
  \begin{itemize}
	\item Множество примитивных функций $G$, состоящее только из унарных
	  и бинарных функций.
	\item Регрессионная выборка $D$.
	\item $N_{max}$ --- максимальное число одновременно рассматриваемых
	  суперпозиций.
	\item $I_{max}$ --- максимальное число итераций алгоритма.
	\item $\hat{Q}$ --- минимальная приспособленность суперпозиций.
  \end{itemize}
\end{algo}

\begin{enumerate}
  \item Инициализируется начальный массив $\mathcal{X}_f$ суперпозиций.
	А именно, порождается $N_{max}$ суперпозиций алгоритмом $\mathcal{RF}$.
  \item Оптимизируются параметры $\boldsymbol{\omega}$ суперпозиций
	из $\mathcal{X}_f$ алгоритмом $\mathcal{LM}$.
  \item Оценивается каждая еще не оцененная суперпозиция $f$ из
	$\mathcal{X}_f$: для нее рассчитывается значение функции ошибки $S_f$
	согласно \eqref{eq:sse_expr} на выборке $D$, и~ставится в~соответствие
	значение $Q_f$ в соответствии с \eqref{eq:q_f}.
  \item Массив суперпозиций $\mathcal{X}_f$ сортируется согласно их
	приспособленности.
  \item Наименее приспособленные суперпозиции удаляются из массива
	$\mathcal{X}_f$ до тех пор, пока его размер не станет равен $N_{max}$.
  \item Отбирается некоторая часть наименее приспособленных суперпозиций из
	$\mathcal{X}_f$ (в данной работе --- $\frac{1}{3}$ от числа всех суперпозиций).
	У этой части происходит случайная замена одной функции
	или свободной переменной на другую: генерируются две случайные величины,
	одна из которых служит для выбора вершины дерева $\Gamma_f$, которую
	предстоит изменить, а другая --- для выбора нового элемента для этой вершины.
	Замена такова, чтобы сохранилась структура суперпозиции, а именно ---
	в~случае замены функции сохраняется арность, а свободная переменная
	заменяется только на другую свободную переменную. При этом исходные
	суперпозиции сохраняются в~массиве $\mathcal{X}_f$.
  \item Повторяются шаги $3-4$.
  \item Производится случайный обмен поддеревьями наиболее приспособленных
	суперпозиций. Вершины, соответствующие этим поддеревьям, выбираются
	случайным образом. При этом исходные суперпозиции сохраняются в~массиве
	$\mathcal{X}_f$.
  \item Повторяются шаги $2-4$.
  \item Проверяются условия останова: если либо число итераций больше
	$I_{max}$, либо в~массиве $\mathcal{X}_f$ есть хотя бы одна суперпозиция с
	приспособленностью больше, чем $\hat{Q}$, то алгоритм останавливается,
	и результатом является наиболее приспособленная суперпозиция, иначе
	переход к~шагу $2$.
\end{enumerate}

Заметим, что мы не делим выборку $D$ на обучающую и контролирующую, оставляя
различные методики контроля качества алгоритма стандартным методикам типа
скользящего контроля.

\section{Вычислительный эксперимент}

\subsection{Понятие волатильности}

Волатильность $\sigma$ --- финансовый показатель, характеризующий
изменчивость цены. Волатильность является важным финансовым показателем
и используется в~управлении финансовыми рисками, так как представляет собой
меру риска использования финансового инструмента (некоторого финансового
документа, передача которого обеспечивает получение денежных средств) за
некоторый заданный промежуток времени.

Волатильность пропорциональна стандартному отклонению $\sigma_{SD}$ стоимости
финансового инструмента и~обратно пропорциональна квадратному корню из
временного периода, обычно измеряемого в~годах:
\[
\sigma = \frac{\sigma_{SD}}{\sqrt{P}}.
\]

Если $P$ измеряется в~годах, то $\sigma$ называется среднегодовой
волатильностью, и~волатильность $\sigma_T$ за интервал времени $T$,
выраженный в~годах, рассчитывается по формуле:
\[
\sigma_T = \sigma \sqrt{T}.
\]

Например, если стандартное отклонение стоимости в~течение дня составляет
$0.01$, а в~году $252$ торговых дня, то волатильность будет равна:
\[
\sigma = \frac{0.01}{\sqrt{\frac{1}{252}}} \approx 0.159.
\]

Отсюда волатильность за месяц будет равна:
\[
\sigma_M = 0.159 \sqrt{\frac{1}{12}} \approx 0.0459.
\]

\subsection{Данные}

В~вычислительном эксперименте используются исторические данные о
волатильности опционов Brent Crude Oil. Срок действия опциона --- полгода,
с 02.01.2001 по 26.06.2001, тип --- право на продажу базового инструмента.
Базовым инструментом в~данном случае является нефть. Использовались
ежедневные цены закрытия опциона и~базового инструмета.

Данный инструмент имеет низкую волатильность, вследствие чего среди данных
нет выбросов. В~данных имется пропуски, так как опционы с ценами, далекими
от цен базового инструмента, не торговались сразу после выпуска опционов.

Использованные данные визуализированы на рис. \ref{fig:raw_data_1}.

\begin{figure}[h]
  \includegraphics[scale=0.75,angle=-90]{figs/raw_data_1.eps}
  \caption{Регрессионная выборка данных}
  \label{fig:raw_data_1}
\end{figure}

\section{Заключение}

В~работе исследованы индуктивные алгоритмы порождения допустимых существенно
нелинейных суперпозиций. Предложен переборный алгоритм, порождающий все
возможные суперпозиции заданной сложности за конечное число шагов, и~приведено
его теоретическое обоснование. 

Алгоритм индуктивного порождения моделей, предложенный в~настоящей работе,
решает некоторые типичные проблемы предложенных ранее методов, упомянутые,
например, в~\cite{Zelinka2008}, а~именно:
\begin{itemize}
  \item Порождение рекурсивных суперпозиций, суперпозиций, содержащих
	несоответствующее используемым функциям число аргументов, и~т. д. --- в
	предложенном алгоритме эти проблемы не возникают по построению.
  \item Несовпадение области определения некоторой примитивной функции и области
	значений ее аргументов (возможно, тоже некоторых суперпозиций).
  \item При ограничении числа примитивных функций, участвующих в~суперпозиции,
	а также при соответствующем задании множества примитивных функций
	исключается проблема слишком сложных суперпозиций.
\end{itemize}

Описан стохастический алгоритм индуктивного порождения существенно нелинейных
суперпозиций и приведены результаты его работы для задачи моделирования
волатильности опционов.

\bibliographystyle{unsrt}
\extrasrussian
\bibliography{bibliography}

\end{document}
