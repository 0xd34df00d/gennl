% This file was created with JabRef 2.6.
% Encoding: UTF8

@BOOK{Aho86,
  title = {Compilers, Principles, Techniques, and Tools},
  publisher = {Addison-Wesley},
  year = {1986},
  author = {Alfred V. Aho and Ravi Sethi and Jeffrey D. Ullman},
  isbn = {0-201-10088-6},
  keywords = {COMPILERS}
}

@ARTICLE{Barmpalexis201175,
  author = {P. Barmpalexis and K. Kachrimanis and A. Tsakonas and E. Georgarakis},
  title = {Symbolic regression via genetic programming in the optimization of
	a controlled release pharmaceutical formulation},
  journal = {Chemometrics and Intelligent Laboratory Systems},
  year = {2011},
  volume = {107},
  pages = {75--82},
  number = {1},
  abstract = {Symbolic regression via genetic programming (GP) was used in the optimisation
	of a pharmaceutical zero-order release matrix tablet, and its predictive
	performance was compared to that of artificial neural network (ANN)
	models. Two types of GP algorithms were employed: 1) standard GP,
	where a single population is used with a restricted or an extended
	function set, and 2) multi-population (island model) GP, where a
	finite number of populations is adopted. The amounts of four polymers,
	namely PEG4000, PVP K30, HPMC K100 and HPMC E50LV were selected as
	independent variables, while the percentage of nimodipine released
	in 2 and 8 h (Y2h, and Y8h), respectively, and the time at which
	90% of the drug was dissolved (t90%), were selected as responses.
	Optimal models were selected by minimisation of the Euclidian distance
	between predicted and optimum release parameters. It was found that
	the prediction ability of GP on an external validation set was higher
	compared to that of the ANNs, with the multi population and standard
	GP combined with an extended function set, showing slightly better
	predictive performance. Similarity factor (f2) values confirmed GP's
	increased prediction performance for multi-population GP (f2 = 85.52)
	and standard GP using an extended function set (f2 = 84.47).},
  doi = {doi:10.1016/j.chemolab.2011.01.012},
  doi-url = {http://dx.doi.org/10.1016/j.chemolab.2011.01.012},
  issn = {0169-7439},
  keywords = {genetic algorithms, genetic programming, Artificial neural networks,
	Controlled release, Experimental design, Optimisation},
  url = {http://www.sciencedirect.com/science/article/B6TFP-523CDG2-4/2/67c4e87b7f04a0e4f5f6fe07a1127ef8}
}

@INPROCEEDINGS{davidson:2000:snrea,
  author = {J. W. Davidson and D. A. Savic and G. A. Walters},
  title = {Symbolic and numerical regression: experiments and applications},
  booktitle = {Developments in Soft Computing},
  year = {2001},
  editor = {Robert John and Ralph Birkenhead},
  pages = {175--182},
  address = {De Montfort University, Leicester, UK},
  month = {29-30 } # jun # { 2000.},
  publisher = {Physica Verlag},
  abstract = {This paper describes a new method for creating polynomial regression
	models. The new method is compared with stepwise regression and symbolic
	regression using three example problems. The first example is a polynomial
	equation. The two examples that follow are real-world problems, approximating
	the Colebrook-White equation and rainfall-runoff modelling},
  isbn = {3-7908-1361-3},
  keywords = {genetic algorithms, genetic programming, least-squares, rule-based
	programming, stepwise regression, symbolic regression}
}

@INCOLLECTION{Fieldsend06,
  author = {Jonathan E. Fieldsend},
  title = {Regression {E}rror {C}haracteristic {O}ptimisation of {N}on-{L}inear
	{M}odels},
  booktitle = {Multi-Objective Machine Learning},
  publisher = {Springer. Studies in Computational Intelligence, Volume 16},
  year = {2006},
  editor = {Yaochu Jin},
  pages = {103--123}
}

@INPROCEEDINGS{Koza1998GP,
  author = {John R. Koza},
  title = {Genetic programming},
  booktitle = {Encyclopedia of Computer Science and Technology},
  year = {1998},
  editor = {James G. Williams and Allen Kent},
  volume = {39},
  pages = {29--43},
  publisher = {Marcel-Dekker},
  note = {Supplement 24},
  keywords = {genetic algorithms, genetic programming},
  notes = {This is summary of genetic programming for Encyclopedia of Computer
	Science and Technology edited by Allen Kent and James G. Williams.
	Good quick survey of GP (koza and others work).},
  size = {On line version 26 pages},
  url = {http://www.genetic-programming.com/jkpdf/encyclopedia1998.pdf}
}

@MISC{Koza1998Intro,
  author = {John R. Koza},
  title = {Introduction to Genetic Algorithms},
  month = aug # {~15},
  year = {1998},
  abstract = {Introduction to Genetic Algorithms John Holland's pioneering book
	Adaptation in Natural and Artificial Systems [1975, 1992] showed
	how the evolutionary process can be applied to solve a wide variety
	of problems using a highly parallel technique that is now called
	the genetic algorithm. The genetic algorithm transforms a population
	of individual objects, each with an associated fitness value, into
	a new generation of the population using the Darwinian principle
	of reproduction and survival of the fittest and naturally occurring
	genetic operations such as crossover (recombination) and mutation.
	Each individual in the population represents a possible solution
	to a given problem. The genetic algorithm attempts to find a very
	good or best solution to the problem by genetically breeding the
	population of individuals. In preparing to u},
  bibsource = {OAI-PMH server at cs1.ist.psu.edu},
  citeseer-references = {oai:CiteSeerPSU:15994; oai:CiteSeerPSU:191576; oai:CiteSeerPSU:551960;
	oai:CiteSeerPSU:550869},
  language = {en},
  oai = {oai:CiteSeerPSU:39917},
  rights = {unrestricted},
  url = {http://citeseer.ist.psu.edu/39917.html; http://www.genetic-programming.com/AiGP1INTRO.ps}
}

@BOOK{B228,
  title = {Categories for the {W}orking {M}athematician},
  publisher = {Springer-Verlag},
  year = {1971},
  author = {S MacLane}
}

@ARTICLE{Marquardt1963Algorithm,
  author = {D. W. Marquardt},
  title = {An Algorithm for Least-squares Estimation of Non-linear Parameters},
  journal = {Journal of the Society of Industrial and Applied Mathematics},
  year = {1963},
  volume = {11},
  pages = {431--441},
  number = {2},
  keywords = {optimization; Levenberg-Marquardt}
}

@BOOK{MathEnc1984_4,
  title = {Математическая энциклопедия},
  publisher = {Советская Энциклопедия},
  year = {1984},
  author = {В.~И.~Битюцков, М.~И.~Войцеховский, А.~Б.~Иванов и др.},
  volume = {4},
  pages = {1214-1215}
}

@INPROCEEDINGS{more:78,
  author = {J. J. Mor\'e},
  title = {The {Levenberg-Marquardt} Algorithm: Implementation and Theory},
  booktitle = {G.A. Watson},
  year = {1978},
  series = {Lecture Notes in Mathematics 630},
  pages = {105--116},
  publisher = {Springer-Verlag, Berlin},
  note = {Cited in {\AA ke Bj\"orck's} bibliography on least squares, which
	is available by anonymous ftp from {\tt math.liu.se} in {\tt pub/references}.},
  kwds = {nlop, lsq, nllsq, Levenberg-Marquardt damping}
}

@ARTICLE{DOI:10.1504/IJCENT.2010.038358,
  author = {Selen Onel and Abe Zeid and Sagar Kamarthi and Meredith Hinds Harris},
  title = {Analysis of risk factors and predictive model for recurrent falls
	in community dwelling older adults},
  journal = {Int. J. of Collaborative Enterprise},
  year = {2011},
  volume = {1},
  pages = {359--380},
  month = feb # {~01},
  abstract = {The objective of this study is to identify the significant risk factors
	of 'fall' among elderly people living in community centres. Different
	factors that may associate with falls are analysed via logistic regression
	(LR) and artificial neural networks (ANNs) models. According to the
	analysis, the most significant predictor in LR analysis is found
	to be the variable 'lost balance within the past year'. Other significant
	predictors are stroke, Hx of dizziness/fainting, fall within past
	30 days, and the number of supplements/vitamins used. The LR model
	gives a prediction accuracy of 76.6\% while an ANN model gives 73\%.
	ANN is expected to provide a more robust computational model than
	LR to analyse past falls and predict future ones. However our research
	results are contrary to those expectations. These results suggest
	that the ANN model need a larger sample size to reach its full potential
	and to give more accurate predictions.},
  bibsource = {OAI-PMH server at www.inderscience.com},
  issn = {1740-2093},
  issue = {3/4},
  language = {eng},
  publisher = {Inderscience Publishers},
  relation = {ISSN online: 1740-2093 ISSN print: 1740-2085},
  rights = {Inderscience Copyright},
  source = {IJCENT (2010), Vol 1 Issue 3/4, pp 359 - 380},
  subject = {elderly people; old people; aged people; age; risk factors; logistic
	regression; artificial neural networks; ANNs; predictive models;
	recurrent falls; communal dwellings; older adults; community centres;
	significant predictors; factor analysis; strokes; dizziness; fainting;
	supplements; vitamins; computational models; sample sizes; Hx; medical
	history; collaborative enterprises; collaboration; healthcare systems;
	systems engineering.},
  url = {http://www.inderscience.com/link.php?id=38358}
}

@INCOLLECTION{reference/ml/X10vc,
  author = {Claude Sammut and Geoffrey I. Webb},
  title = {Symbolic Regression},
  booktitle = {Encyclopedia of Machine Learning},
  publisher = {Springer},
  year = {2010},
  editor = {Claude Sammut and Geoffrey I. Webb},
  pages = {954},
  bibdate = {2011-04-08},
  bibsource = {DBLP, http://dblp.uni-trier.de/db/reference/ml/ml2010.html#X10vc},
  isbn = {978-0-387-30768-8},
  url = {http://dx.doi.org/10.1007/978-0-387-30164-8}
}

@ARTICLE{Shi:2011:CRM,
  author = {Peng Shi and Wei Zhang},
  title = {A copula regression model for estimating firm efficiency in the insurance
	industry},
  journal = {Journal of Applied Statistics},
  year = {2011},
  volume = {38},
  pages = {2271--2287},
  number = {10},
  month = oct,
  acknowledgement = {Nelson H. F. Beebe, University of Utah, Department of Mathematics,
	110 LCB, 155 S 1400 E RM 233, Salt Lake City, UT 84112-0090, USA,
	Tel: +1 801 581 5254, FAX: +1 801 581 4148, e-mail: \path|beebe@math.utah.edu|,
	\path|beebe@acm.org|, \path|beebe@computer.org| (Internet), URL:
	\path|http://www.math.utah.edu/~beebe/|},
  bibdate = {Mon Sep 5 18:53:30 MDT 2011},
  bibsource = {http://www.tandf.co.uk/journals/routledge/02664763.html},
  coden = {????},
  doi = {http://dx.doi.org/10.1080/02664763.2010.545376},
  doi-url = {http://dx.doi.org/10.1080/02664763.2010.545376},
  issn = {0266-4763 (print), 1360-0532 (electronic)},
  issn-l = {0266-4763},
  onlinedate = {18 Aug 2011}
}

@BOOKLET{Strijov08ChoosingMethods,
  title = {Методы выбора регрессионных моделей},
  author = {Strijov, V.},
  year = {2010},
  owner = {d34df00d},
  timestamp = {2011.09.19}
}

@BOOKLET{Strijov08InductMethods,
  title = {Методы индуктивного порождения регрессионных моделей},
  author = {Strijov, V.},
  year = {2008},
  owner = {d34df00d},
  timestamp = {2011.09.19}
}

@ARTICLE{journals/cma/StrijovW10,
  author = {Vadim Strijov and Gerhard-Wilhelm Weber},
  title = {Nonlinear regression model generation using hyperparameter optimization},
  journal = {Computers \& Mathematics with Applications},
  year = {2010},
  volume = {60},
  pages = {981--988},
  number = {4},
  bibdate = {2010-12-20},
  bibsource = {DBLP, http://dblp.uni-trier.de/db/journals/cma/cma60.html#StrijovW10},
  url = {http://dx.doi.org/10.1016/j.camwa.2010.03.021}
}

@BOOKLET{Strijov08InductMethods,
  title = {Алгоритмы поиска суперпозиций при выборе оптимальных регрессионных
	моделей},
  author = {Strijov, V., Ptashko, G.},
  year = {2007},
  owner = {d34df00d},
  timestamp = {2011.09.19}
}

@INPROCEEDINGS{Torres-Trevino:2011:GECCOcomp,
  author = {Luis M. Torres-Trevino},
  title = {Symbolic regression using {\$\alpha\$}- {\$\beta\$} operators and
	estimation of distribution algorithms: preliminary results},
  booktitle = {3rd symbolic regression and modeling workshop for GECCO 2011},
  year = {2011},
  editor = {Steven Gustafson and Ekaterina Vladislavleva},
  pages = {647--654},
  address = {Dublin, Ireland},
  month = {12-16 } # jul,
  publisher = {ACM},
  abstract = {Modelling processes is an important task in engineering; however,
	the generation of models using only experimental data is not a straightforward
	problem. Linear regression, neural networks, and other approaches
	have been used for this purpose; nevertheless, a mathematical description
	is desirable specially when an optimisation is required. Symbolic
	regression has been used for generating equations considering only
	experimental data. In this paper, two new operators are proposed
	to represent a mathematical model of a process. These operators simplified
	the way for representing equations making possible its use as a symbolic
	regression. The correct model is generated selecting the appropriate
	operators and parameters using an evolutionary algorithm like the
	estimation of distribution algorithms. As a preliminary results,
	three cases are used to illustrated the performance of the proposed
	approach. The results indicates that the use of these alpha, beta
	operators are a promising way to apply symbolic regression to model
	complex process.},
  doi = {doi:10.1145/2001858.2002062},
  doi-url = {http://dx.doi.org/10.1145/2001858.2002062},
  isbn = {978-1-4503-0690-4},
  notes = {Also known as \cite{2002062} Distributed on CD-ROM at GECCO-2011.
	ACM Order Number 910112.},
  organisation = {SIGEVO},
  publisher_address = {New York, NY, USA}
}

@MISC{Zelinka2008,
  author = {Ivan Zelinka and Zuzana Oplatkova and Lars Nolle},
  title = {{I}. {ZELINKA} et al: {ANALYTICAL} {PROGRAMMING} {\ldots} {ANALYTIC}
	{PROGRAMMING} -- {SYMBOLIC} {REGRESSION} {BY} {MEANS} {OF} {ARBITRARY}
	{EVOLUTIONARY} {ALGORITHMS}},
  month = aug # {~14},
  year = {2008},
  abstract = {This contribution introduces analytical programming, a novel method
	that allows solving various problems from the symbolic regression
	domain. Symbolic regression was first proposed by J. R. Koza in his
	genetic programming and by C. Ryan in grammatical evolution. This
	contribution explains the main principles of analytic programming,
	and demonstrates its ability to synthesize suitable solutions, called
	programs. It is then compared in its structure with genetic programming
	and grammatical evolution. After theoretical part, a comparative
	study concerned with Boolean k-symmetry and k-even problems from
	Koza{'}s genetic programming domain is done with analytical programming.
	Here, two evolutionary algorithms are used with analytical programming:
	differential evolution and self-organizing migrating algorithm. Boolean
	k-symmetry and k-even problems comparative study here are continuation
	of previous comparative studies done by analytic programming in the
	past.},
  bibsource = {OAI-PMH server at citeseerx.ist.psu.edu},
  contributor = {CiteSeerX},
  language = {en},
  number = {9/},
  oai = {oai:CiteSeerXPSU:10.1.1.116.9558},
  relation = {10.1.1.100.6544; 10.1.1.38.7697},
  rights = {Metadata may be used without restrictions as long as the oai identifier
	remains attached to it.},
  subject = {symbolic regression; genetic programming; grammar evolution; analytic
	programming; SOMA},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.116.9558; http://ducati.doc.ntu.ac.uk/uksim/journal/vol-6/no.9/paper5.pdf}
}

@BOOK{Pavlovsky2000,
  title = {Имитационные модели и системы},
  publisher = {Фазис},
  year = {2000},
  author = {Ю. Н. Павловский},
  owner = {d34df00d},
  timestamp = {2011.10.18}
}

@comment{jabref-meta: selector_publisher:}

@comment{jabref-meta: selector_author:}

@comment{jabref-meta: selector_journal:}

@comment{jabref-meta: selector_keywords:}

